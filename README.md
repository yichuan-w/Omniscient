---
title: Omniscient
emoji: ğŸ‘ï¸â€ğŸ—¨ï¸
colorFrom: indigo
colorTo: purple
sdk: streamlit
python_version: 3.11
sdk_version: "1.35.0"
app_file: app.py
pinned: false
---

<div align="center">

# ğŸ§  Omniscient 
### *"The all-knowing AI that sees everything, knows everything"*

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.35.0-red.svg)](https://streamlit.io)
[![HuggingFace](https://img.shields.io/badge/ğŸ¤—%20HuggingFace-Space-yellow.svg)](https://huggingface.co/spaces/Omniscient001/Omniscient)

*A versatile AI bot for image analysis and dataset curation with support for multiple AI models*

ğŸ® **[Try it Live on HuggingFace!](https://huggingface.co/spaces/Omniscient001/Omniscient)** *(Actively WIP)*

</div>

---

## âœ¨ Features

<table>
<tr>
<td width="50%">

### ğŸ—ƒï¸ **Dataset Curation**
Generate and curate high-quality image datasets with intelligent filtering and categorization.

### ğŸ” **Image Analysis with multiple angle** 
Benchmark different AI models on individual images with detailed performance metrics.

</td>
<td width="50%">

### ğŸ¤– **Agentic Analysis**
Multi-step AI reasoning and analysis with advanced decision-making capabilities.

### ğŸŒ **Multiple AI Providers**
Seamless integration with OpenAI, Anthropic, and Google AI platforms.

</td>
</tr>
</table>

---

## ğŸš€ Quick Start

### ğŸ“‹ **Step 1: Setup Environment**

```bash
cd simple_G_ai_bot
```

Create a `.env` file in the project root:

```bash
# ğŸ” .env
OPENAI_API_KEY=your_openai_api_key_here
ANTHROPIC_API_KEY=your_anthropic_api_key_here
GOOGLE_API_KEY=your_google_api_key_here
```

### ğŸ“¦ **Step 2: Install Dependencies**

```bash
uv sync
```

### ğŸ¯ **Step 3: Usage Examples**

<details>
<summary><b>ğŸ—ï¸ Data Collection (Collect Mode)</b></summary>

Generate 50 samples into a named dataset (thumbnails + labels):
```bash
python main.py --mode collect --dataset my_dataset --samples 50 --headless
```

</details>

<details>
<summary><b>âš¡ Single Image Analysis (Benchmark Mode)</b></summary>

Benchmark GPT-4o on 5 samples:
```bash
python main.py --mode benchmark --models gpt-4o --samples 5
```

</details>

<details>
<summary><b>ğŸ§  Agentic Analysis (Agent Mode)</b></summary>

Run multi-step analysis with Gemini:
```bash
python main.py --mode agent --model gemini-2.5-pro --steps 10 --samples 5
```

</details>

<details>
<summary><b>ğŸ”¬ Multi-Run, Per-Step Evaluation (Test Mode)</b></summary>

Run per-step evaluation across models with multiple runs and logging:
```bash
python main.py --mode test --models gpt-4o claude-3-7-sonnet --dataset test --steps 10 --samples 30 --runs 5
```

Outputs per-step accuracy and average distance, and saves logs under `results/test/<timestamp>/<model>/`.

</details>

---

## âš™ï¸ Configuration

### ğŸ”‘ **Environment Variables**

| Variable | Description | Status |
|:---------|:------------|:------:|
| `OPENAI_API_KEY` | OpenAI API key for GPT models | ğŸ”¶ Optional |
| `ANTHROPIC_API_KEY` | Anthropic API key for Claude models | ğŸ”¶ Optional |
| `GOOGLE_API_KEY` | Google AI API key for Gemini models | ğŸ”¶ Optional |

### ğŸ› ï¸ **Command Line Options**

#### ğŸŒŸ **Common Options**
- `--mode` â†’ Operation mode (`agent`, `benchmark`, `collect`, `test`)
- `--dataset` â†’ Dataset name to use or create *(default: `default`)*
- `--samples` â†’ Number of samples to process *(default: 50)*
- `--headless` â†’ Run browser in headless mode

#### ğŸ“Š **Benchmark Mode**
- `--models` â†’ One or more models, e.g. `--models gpt-4o claude-3-7-sonnet`
- `--temperature` â†’ LLM sampling temperature *(default: 0.0)*

#### ğŸ¤– **Agent Mode**
- `--model` â†’ Single model, e.g. `--model gemini-2.5-pro`
- `--steps` â†’ Max reasoning steps *(default: 10)*
- `--temperature` â†’ LLM sampling temperature *(default: 0.0)*

#### ğŸ”¬ **Test Mode**
- `--models` â†’ One or more models to compare
- `--steps` â†’ Max steps per sample *(records per-step metrics)*
- `--runs` â†’ Repeats per model to stabilize metrics *(default: 3)*
- `--temperature` â†’ LLM sampling temperature *(default: 0.0)*
- `--id` â†’ Run only the sample with this specific ID *(e.g., `--id 09ce31a1-a719-4ed9-a344-7987214902c1`)*

---

## ğŸ¯ Supported Models

<div align="center">

| Provider | Models | Status |
|:--------:|:-------|:------:|
| **ğŸ”µ OpenAI** | GPT-4o, GPT-4, GPT-3.5-turbo | âœ… Active |
| **ğŸŸ£ Anthropic** | Claude-3-opus, Claude-3-sonnet, Claude-3-haiku | âœ… Active |
| **ğŸ”´ Google** | Gemini-2.5-pro, Gemini-pro, Gemini-pro-vision | âœ… Active |

</div>

---

## ğŸ“‹ Requirements

> **Prerequisites:**
> - ğŸ Python 3.8+
> - ğŸ“¦ UV package manager  
> - ğŸ”‘ Valid API keys for desired AI providers

---

## ğŸ”§ Installation

<table>
<tr>
<td>

**1ï¸âƒ£** Clone the repository
```bash
git clone <repository-url>
```

**2ï¸âƒ£** Navigate to project directory
```bash
cd simple_G_ai_bot
```

</td>
<td>

**3ï¸âƒ£** Create `.env` file with your API keys
```bash
touch .env
# Add your API keys
```

**4ï¸âƒ£** Install dependencies
```bash
uv sync
```

</td>
</tr>
</table>

**5ï¸âƒ£** Run the bot with desired mode and options! ğŸ‰

---

## ğŸ’¡ Examples

### ğŸ—ï¸ **Basic Data Collection**
```bash
python main.py --mode collect --dataset my_dataset --samples 20 --headless
```

### âš”ï¸ **Model Comparison (Benchmark)**
```bash
# GPT-4o Analysis
python main.py --mode benchmark --models gpt-4o --samples 10

# Claude-3 Analysis  
python main.py --mode benchmark --models claude-3-opus --samples 10
```

### ğŸ§  **Advanced Agentic Workflow**
```bash
python main.py --mode agent --model gemini-2.5-pro --steps 15 --samples 3
```

### ğŸ”¬ **Per-Step Curves and Logs (Test Mode)**
```bash
python main.py --mode test --models gpt-4o gemini-2.5-pro --dataset test --steps 10 --samples 30 --runs 5
```

This saves JSON logs per model in `results/test/<timestamp>/<model>/` and prints per-step accuracy and average distance.

**Single Sample Testing**: To test a specific sample by ID:
```bash
python main.py --mode test --models gpt-4o --dataset test --steps 10 --runs 3 --id 09ce31a1-a719-4ed9-a344-7987214902c1
```

**Quick ID Test**: Run a specific sample by ID(like: 09ce31a1-a719-4ed9-a344-7987214902c1):
```bash
python main.py --mode test --models gpt-4o --dataset test --steps 10 --runs 3 --id <sample_id>
```

---

## ğŸ§­ Modes

- **Agent**: Multi-step agent that explores and then makes a final guess. Uses a simpler prompt for action selection and a final `GUESS` at the last step. Good for end-to-end agent behavior.
- **Benchmark**: Single-image baseline (no multi-step exploration). Good for quick, pure recognition comparisons between models.
- **Test**: Multi-model, multi-run evaluation that records a prediction at every step and logs detailed results. Produces per-step accuracy and average-distance curves; logs saved under `results/test/<timestamp>/<model>/`.
- **Collect**: Generates datasets by sampling locations from MapCrunch and saving `datasets/<name>/golden_labels.json` plus thumbnails. Use this for data generation.

---

## ğŸ” Security Note

> âš ï¸ **Important**: Never commit your `.env` file to version control. Add `.env` to your `.gitignore` file to keep your API keys secure.

---

<div align="center">

## ğŸ“œ License

**MIT License** - see [LICENSE](LICENSE) file for details.

---

<img src="https://img.shields.io/badge/Made%20with-â¤ï¸-red.svg" alt="Made with love">
<img src="https://img.shields.io/badge/AI%20Powered-ğŸ¤–-blue.svg" alt="AI Powered">
<img src="https://img.shields.io/badge/Open%20Source-ğŸ’š-green.svg" alt="Open Source">

**â­ Star this repo if you find it useful!**

</div>